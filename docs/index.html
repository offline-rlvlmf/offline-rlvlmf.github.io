<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Real-World Offline Reinforcement Learning from Vision Language Model Feedback">
  <meta name="keywords" content="Offline RL, Vision-Language Models, Reward Learning, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Offline RL-VLM-F</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./images/einstein-logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Real-World Offline Reinforcement Learning from Vision Language Model Feedback</h1>
          <h1 class="title is-2 conference-title">In Submission at ICRA 2025</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sreyas Venkataraman<sup>1, 3*</sup>,</span>
            <span class="author-block">Yufei Wang<sup>1*</sup>,</span>
            <span class="author-block">Ziyu Wang<sup>2</sup>,</span>
            <span class="author-block">Zackory Erickson<sup>1†</sup>,</span>
            <span class="author-block">David Held<sup>1†</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Robotics Institute, Carnegie Mellon University,</span>
            <span class="author-block"><sup>2</sup>IIIS, Tsinghua University</span>
            <span class="author-block"><sup>3</sup>Indian Institute of Technology, Kharagpur</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Update links to actual resources -->
              <span class="link-block">
                <a href="./ICRA25_ICRA_OfflineRewardLearning-5.pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              <span class="link-block">
                <a href="https://github.com/user/repo" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <video controls style="border: 2px solid gray; border-radius: 15px; box-shadow: 0px 0px 10px #999; padding: 5px; max-width: 100%;">
        <source src="./videos/summary_vid.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <h2 class="subtitle has-text-centered">
        Combining preference based reward learning from Vision-Language Models with offline RL for policy learning from unlabeled datasets
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size: 18px;">
           Offline reinforcement learning can enable policy learning from pre-collected, sub-optimal datasets without online interactions. This makes it ideal for real-world robots and safety-critical scenarios, where collecting online data or expert demonstrations is slow, costly, and risky. However, most existing offline RL works assume the dataset is already labeled with the task rewards, a process that often requires significant human effort, especially when ground-truth states are hard to ascertain
          (e.g., in the real-world). In this paper, we build on prior work, specifically RL-VLM-F, and propose a novel system that automatically generates reward labels for offline datasets using preference feedback from a vision-language model and a text description of the task. Our method then learns a policy using offline RL with the reward-labeled dataset. We demonstrate the
          system’s applicability to a complex real-world robot-assisted dressing task, where we first learn a reward function using a vision-language model on a sub-optimal offline dataset, and then we use the learned reward to employ Implicit Q learning to
          develop an effective dressing policy. Our method also performs well in simulation tasks involving the manipulation of rigid and deformable objects, and significantly outperform baselines such as behavior cloning and inverse RL. In summary, we propose a new system that enables automatic reward labeling and policy
          learning from unlabeled, sub-optimal offline datasets.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- Add more sections for methodology, experiments, etc., as per the paper -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-centered">
          <img src="./images/Offline RL-VLM-F method.png" alt="Methods Figure" style="border: 2px solid gray; border-radius: 15px; box-shadow: 0px 0px 10px #999; padding: 5px; max-width: 100%;">
          <p class="subtitle has-text-centered" style="margin-top: 10px;">
            Top: Our system, Offline RL-VLM-F, combines RL-VLM-F [1] with offline RL for effective policy learning from
unlabeled datasets. Given an unlabeled dataset without rewards, it first samples observation pairs, and queries a Vision
Language Model for preferences given a text description of the task. Using the preference labels, it then learns a reward
model via preference-based reward learning. The learned reward model is utilized to label the entire offline dataset. Finally,
it performs offline RL with the labeled dataset to learn a policy that solves the task. Bottom: we follow the same VLM
querying process as in RL-VLM-F. It consists of two stages: the first is an analysis stage that asks the VLM to analyze
and compare the two images; the second is the labeling stage, where the VLM generates the preference labels based on its
own analysis from the first stage and the task description.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
  
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="./ICRA25_ICRA_OfflineRewardLearning-5.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/user/repo" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the template from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website under the <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
